{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32481c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676efc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea4641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2039695\n",
      "Test samples: 509924\n",
      "Columns: ['primary_subject', 'subjects', 'abstract', 'title']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "from mlops_project.data import ArxivPapersDataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Load the dataset using the project's data module\n",
    "data_dir = Path(\"../data\")\n",
    "train_dataset = ArxivPapersDataset(split=\"train\", data_dir=data_dir).dataset\n",
    "test_dataset = ArxivPapersDataset(split=\"test\", data_dir=data_dir).dataset\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2jq29ptdehk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "\n",
    "def create_contrastive_pairs(dataset, num_pairs: int = 100000, text_field: str = \"abstract\", seed: int = 42):\n",
    "    \"\"\"\n",
    "    Create balanced positive and negative pairs for ContrastiveLoss.\n",
    "\n",
    "    Returns a dataset with columns: sentence1, sentence2, label\n",
    "    - label=1.0 for positive pairs (same subject)\n",
    "    - label=0.0 for negative pairs (different subjects)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Group indices by subject\n",
    "    subject_to_indices = defaultdict(list)\n",
    "    for idx, subject in enumerate(dataset[\"primary_subject\"]):\n",
    "        subject_to_indices[subject].append(idx)\n",
    "\n",
    "    subjects = list(subject_to_indices.keys())\n",
    "    print(f\"Found {len(subjects)} unique subjects\")\n",
    "\n",
    "    pairs = {\"sentence1\": [], \"sentence2\": [], \"label\": []}\n",
    "    num_positive = num_pairs // 2\n",
    "    num_negative = num_pairs - num_positive\n",
    "\n",
    "    # Create positive pairs (same subject)\n",
    "    print(f\"Creating {num_positive} positive pairs...\")\n",
    "    for _ in range(num_positive):\n",
    "        subject = random.choice([s for s in subjects if len(subject_to_indices[s]) >= 2])\n",
    "        idx1, idx2 = random.sample(subject_to_indices[subject], 2)\n",
    "        pairs[\"sentence1\"].append(dataset[idx1][text_field])\n",
    "        pairs[\"sentence2\"].append(dataset[idx2][text_field])\n",
    "        pairs[\"label\"].append(1.0)\n",
    "\n",
    "    # Create negative pairs (different subjects)\n",
    "    print(f\"Creating {num_negative} negative pairs...\")\n",
    "    for _ in range(num_negative):\n",
    "        subj1, subj2 = random.sample(subjects, 2)\n",
    "        idx1 = random.choice(subject_to_indices[subj1])\n",
    "        idx2 = random.choice(subject_to_indices[subj2])\n",
    "        pairs[\"sentence1\"].append(dataset[idx1][text_field])\n",
    "        pairs[\"sentence2\"].append(dataset[idx2][text_field])\n",
    "        pairs[\"label\"].append(0.0)\n",
    "\n",
    "    return Dataset.from_dict(pairs)\n",
    "\n",
    "\n",
    "def load_or_create_pairs(dataset, save_path: Path, num_pairs: int, text_field: str = \"abstract\", seed: int = 42):\n",
    "    \"\"\"Load pairs from disk if they exist, otherwise create and save them.\"\"\"\n",
    "    if save_path.exists():\n",
    "        print(f\"Loading cached pairs from {save_path}\")\n",
    "        return load_from_disk(str(save_path))\n",
    "\n",
    "    print(f\"Creating new pairs (will be cached at {save_path})\")\n",
    "    pairs = create_contrastive_pairs(dataset, num_pairs=num_pairs, text_field=text_field, seed=seed)\n",
    "    pairs.save_to_disk(str(save_path))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed3d5280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new pairs (will be cached at ../data/train_pairs_cl_1000000)\n",
      "Found 148 unique subjects\n",
      "Creating 500000 positive pairs...\n",
      "Creating 500000 negative pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff5d94252eb4560b1a6f8218ee9a9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training pairs: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Create/load training pairs\n",
    "train_pairs_path = data_dir / \"train_pairs_cl_1000000\"\n",
    "train_pairs = load_or_create_pairs(train_dataset, train_pairs_path, num_pairs=1000000, seed=42)\n",
    "print(f\"\\nTraining pairs: {len(train_pairs)}\")\n",
    "# print(train_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6uqsqbt5oik",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached pairs from ../data/eval_pairs_cl\n",
      "Evaluation pairs: 10000\n"
     ]
    }
   ],
   "source": [
    "# Create/load evaluation pairs\n",
    "eval_pairs_path = data_dir / \"eval_pairs_cl\"\n",
    "eval_pairs = load_or_create_pairs(test_dataset, eval_pairs_path, num_pairs=10000, seed=42)\n",
    "print(f\"Evaluation pairs: {len(eval_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "t00b0e1dfap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR Evaluator: 996 queries, 4000 corpus docs (no overlap)\n",
      "IR Evaluator created for precision@k metrics\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "def create_ir_evaluator(dataset, sample_size: int = 5000, name: str = \"arxiv-retrieval\"):\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(len(dataset), min(sample_size, len(dataset)), replace=False)\n",
    "    \n",
    "    # Split indices: first 20% for queries, rest for corpus (no overlap)\n",
    "    num_queries = sample_size // 5\n",
    "    query_indices = indices[:num_queries]\n",
    "    corpus_indices = indices[num_queries:]\n",
    "    \n",
    "    queries = {}\n",
    "    corpus = {}\n",
    "    relevant_docs = {}\n",
    "    subject_to_corpus_ids = defaultdict(set)\n",
    "    \n",
    "    # Build corpus from corpus_indices only\n",
    "    for i, idx in enumerate(corpus_indices):\n",
    "        idx = int(idx)\n",
    "        corpus_id = f\"doc_{i}\"\n",
    "        corpus[corpus_id] = dataset[idx][\"abstract\"]\n",
    "        subject = dataset[idx][\"primary_subject\"]\n",
    "        subject_to_corpus_ids[subject].add(corpus_id)\n",
    "    \n",
    "    # Build queries from query_indices (no overlap with corpus)\n",
    "    for i, idx in enumerate(query_indices):\n",
    "        idx = int(idx)\n",
    "        query_id = f\"query_{i}\"\n",
    "        queries[query_id] = dataset[idx][\"abstract\"]\n",
    "        subject = dataset[idx][\"primary_subject\"]\n",
    "        # Relevant docs are corpus docs with same subject\n",
    "        relevant_docs[query_id] = subject_to_corpus_ids[subject].copy()\n",
    "    \n",
    "    # Filter out queries with no relevant docs\n",
    "    queries = {qid: q for qid, q in queries.items() if len(relevant_docs.get(qid, set())) > 0}\n",
    "    relevant_docs = {qid: docs for qid, docs in relevant_docs.items() if qid in queries}\n",
    "    \n",
    "    print(f\"IR Evaluator: {len(queries)} queries, {len(corpus)} corpus docs (no overlap)\")\n",
    "    \n",
    "    return InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=name,\n",
    "        precision_recall_at_k=[1, 5, 10],\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "ir_evaluator = create_ir_evaluator(test_dataset, sample_size=5000)\n",
    "print(\"IR Evaluator created for precision@k metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61jd6zeqsq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Precision@k (before fine-tuning) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263cffd027fe4fb4ab3cb09e25d5ce4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc8313830b946099bb0101a927fd5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv-retrieval_cosine_accuracy@1: 0.4900\n",
      "arxiv-retrieval_cosine_accuracy@3: 0.7430\n",
      "arxiv-retrieval_cosine_accuracy@5: 0.8153\n",
      "arxiv-retrieval_cosine_accuracy@10: 0.8815\n",
      "arxiv-retrieval_cosine_precision@1: 0.4900\n",
      "arxiv-retrieval_cosine_precision@5: 0.4466\n",
      "arxiv-retrieval_cosine_precision@10: 0.4149\n",
      "arxiv-retrieval_cosine_recall@1: 0.0115\n",
      "arxiv-retrieval_cosine_recall@5: 0.0453\n",
      "arxiv-retrieval_cosine_recall@10: 0.0767\n",
      "arxiv-retrieval_cosine_ndcg@10: 0.4343\n",
      "arxiv-retrieval_cosine_mrr@10: 0.6277\n",
      "arxiv-retrieval_cosine_map@100: 0.2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline (before fine-tuning)\n",
    "print(\"=== Baseline Precision@k (before fine-tuning) ===\")\n",
    "baseline_results = ir_evaluator(model)\n",
    "for key, value in baseline_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "j9niui5kjqg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0961bd2404394fe79e4e3484458e1f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized with precision@k evaluator. Ready to train!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.losses import ContrastiveLoss, OnlineContrastiveLoss\n",
    "\n",
    "# Define training arguments\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"../models/contrastive-minilm\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    eval_on_start=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    torch_compile=torch.cuda.is_available(),\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    tf32=torch.cuda.is_available())\n",
    "\n",
    "# Initialize loss function\n",
    "loss = OnlineContrastiveLoss(model)\n",
    "\n",
    "# Create trainer with IR evaluator for precision@k\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_pairs,\n",
    "    eval_dataset=eval_pairs,\n",
    "    loss=loss,\n",
    "    evaluator=ir_evaluator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized with precision@k evaluator. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "icdba4mnsyh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthorhojhus\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thorh/MLOpsProject/notebooks/wandb/run-20260109_191036-d9dsmg91</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thorhojhus/sentence-transformers/runs/d9dsmg91' target=\"_blank\">lunar-haze-12</a></strong> to <a href='https://wandb.ai/thorhojhus/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thorhojhus/sentence-transformers' target=\"_blank\">https://wandb.ai/thorhojhus/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thorhojhus/sentence-transformers/runs/d9dsmg91' target=\"_blank\">https://wandb.ai/thorhojhus/sentence-transformers/runs/d9dsmg91</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3122' max='7813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3122/7813 12:40 < 19:04, 4.10 it/s, Epoch 0.40/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Arxiv-retrieval Cosine Accuracy@1</th>\n",
       "      <th>Arxiv-retrieval Cosine Accuracy@3</th>\n",
       "      <th>Arxiv-retrieval Cosine Accuracy@5</th>\n",
       "      <th>Arxiv-retrieval Cosine Accuracy@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Precision@1</th>\n",
       "      <th>Arxiv-retrieval Cosine Precision@5</th>\n",
       "      <th>Arxiv-retrieval Cosine Precision@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Recall@1</th>\n",
       "      <th>Arxiv-retrieval Cosine Recall@5</th>\n",
       "      <th>Arxiv-retrieval Cosine Recall@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Ndcg@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Mrr@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>27.913651</td>\n",
       "      <td>0.489960</td>\n",
       "      <td>0.742972</td>\n",
       "      <td>0.815261</td>\n",
       "      <td>0.881526</td>\n",
       "      <td>0.489960</td>\n",
       "      <td>0.446386</td>\n",
       "      <td>0.414759</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>0.045248</td>\n",
       "      <td>0.076651</td>\n",
       "      <td>0.434230</td>\n",
       "      <td>0.627709</td>\n",
       "      <td>0.200542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.466500</td>\n",
       "      <td>3.126542</td>\n",
       "      <td>0.424699</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.737952</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.424699</td>\n",
       "      <td>0.398795</td>\n",
       "      <td>0.379116</td>\n",
       "      <td>0.008699</td>\n",
       "      <td>0.040405</td>\n",
       "      <td>0.073222</td>\n",
       "      <td>0.391163</td>\n",
       "      <td>0.557256</td>\n",
       "      <td>0.196756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.335500</td>\n",
       "      <td>3.298453</td>\n",
       "      <td>0.383534</td>\n",
       "      <td>0.621486</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.810241</td>\n",
       "      <td>0.383534</td>\n",
       "      <td>0.360843</td>\n",
       "      <td>0.347590</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>0.036649</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>0.358766</td>\n",
       "      <td>0.522611</td>\n",
       "      <td>0.171785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.207000</td>\n",
       "      <td>3.070521</td>\n",
       "      <td>0.423695</td>\n",
       "      <td>0.646586</td>\n",
       "      <td>0.721888</td>\n",
       "      <td>0.821285</td>\n",
       "      <td>0.423695</td>\n",
       "      <td>0.378916</td>\n",
       "      <td>0.364357</td>\n",
       "      <td>0.008599</td>\n",
       "      <td>0.036175</td>\n",
       "      <td>0.069692</td>\n",
       "      <td>0.377143</td>\n",
       "      <td>0.552732</td>\n",
       "      <td>0.200713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.970100</td>\n",
       "      <td>2.994114</td>\n",
       "      <td>0.406627</td>\n",
       "      <td>0.636546</td>\n",
       "      <td>0.723896</td>\n",
       "      <td>0.820281</td>\n",
       "      <td>0.406627</td>\n",
       "      <td>0.384337</td>\n",
       "      <td>0.373193</td>\n",
       "      <td>0.008295</td>\n",
       "      <td>0.037724</td>\n",
       "      <td>0.073362</td>\n",
       "      <td>0.383467</td>\n",
       "      <td>0.543791</td>\n",
       "      <td>0.199251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.883300</td>\n",
       "      <td>3.033103</td>\n",
       "      <td>0.410643</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.714859</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.410643</td>\n",
       "      <td>0.380120</td>\n",
       "      <td>0.366265</td>\n",
       "      <td>0.008147</td>\n",
       "      <td>0.039037</td>\n",
       "      <td>0.071679</td>\n",
       "      <td>0.377875</td>\n",
       "      <td>0.545239</td>\n",
       "      <td>0.197523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.858300</td>\n",
       "      <td>2.785812</td>\n",
       "      <td>0.430723</td>\n",
       "      <td>0.655622</td>\n",
       "      <td>0.744980</td>\n",
       "      <td>0.821285</td>\n",
       "      <td>0.430723</td>\n",
       "      <td>0.395582</td>\n",
       "      <td>0.376205</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.041029</td>\n",
       "      <td>0.074624</td>\n",
       "      <td>0.388525</td>\n",
       "      <td>0.556850</td>\n",
       "      <td>0.215924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7be0207fed746b3b895b073d758369b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611e786c9a2640dd812a9049d46ae14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "W0109 19:12:51.865000 113476 torch/fx/experimental/symbolic_shapes.py:6823] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s47, s8) | Eq(s8, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dab17bde5141ff9932d02e2c9c4fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c6ff736f54447b8d62e03620a41113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6b46771af34b3f9b90e713701aa4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34991dbcc1334facb574206c8faa11a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7857e19130459dafa5ab3a1858d9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba2d5ece7e847548ac6e047dbbfc632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bf09574bdf46b79445ff534e6ef8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5968f8714ac49ea83c29fa658bc6846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954be4f6d08445ee8632fde25fcd4a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99f78f3db3240d790b7b858d92e164a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd73d4afd3134b949251c4c747b64365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b763e6c7bd704077a8c62fd6d17cc0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/sentence_transformers/trainer.py:434\u001b[39m, in \u001b[36mSentenceTransformerTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    429\u001b[39m     model == \u001b[38;5;28mself\u001b[39m.model_wrapped\n\u001b[32m    430\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(loss_fn, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Only if the loss stores the model\u001b[39;00m\n\u001b[32m    431\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m loss_fn.model != model  \u001b[38;5;66;03m# Only if the wrapped model is not already stored\u001b[39;00m\n\u001b[32m    432\u001b[39m ):\n\u001b[32m    433\u001b[39m     loss_fn = \u001b[38;5;28mself\u001b[39m.override_model_in_loss(loss_fn, model)\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.track_loss_components(loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/sentence_transformers/losses/OnlineContrastiveLoss.py:82\u001b[39m, in \u001b[36mOnlineContrastiveLoss.forward\u001b[39m\u001b[34m(self, sentence_features, labels, size_average)\u001b[39m\n\u001b[32m     79\u001b[39m poss = distance_matrix[labels == \u001b[32m1\u001b[39m]\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# select hard positive and hard negative pairs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m negative_pairs = negs[negs < (poss.max() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mposs\u001b[49m\u001b[43m)\u001b[49m > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m negs.mean())]\n\u001b[32m     83\u001b[39m positive_pairs = poss[poss > (negs.min() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(negs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m poss.mean())]\n\u001b[32m     85\u001b[39m positive_loss = positive_pairs.pow(\u001b[32m2\u001b[39m).sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/torch/_tensor.py:1160\u001b[39m, in \u001b[36mTensor.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1157\u001b[39m \u001b[34m__neg__\u001b[39m = _C.TensorBase.neg\n\u001b[32m   1158\u001b[39m \u001b[34m__abs__\u001b[39m = _C.TensorBase.abs\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1162\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.\u001b[34m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7bd354416060>> (for post_run_cell), with arguments args (<ExecutionResult object at 7bd3c1f4fe60, execution_count=10 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7bd3c1f4fdd0, raw_cell=\"# Start training\n",
      "trainer.train()\" transformed_cell=\"# Start training\n",
      "trainer.train()\n",
      "\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bdesktop/home/thorh/MLOpsProject/notebooks/model_train_cl.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:604\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface.py:811\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    810\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLOpsProject/.venv/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/streams.py:392\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yhukcae0z1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Precision@k (after fine-tuning) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4ecd46be3144eda3729bff63af473a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98893a41c95a47f8a32801522814061b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv-retrieval_cosine_precision@1: 0.4538\n",
      "arxiv-retrieval_cosine_precision@5: 0.4102\n",
      "arxiv-retrieval_cosine_precision@10: 0.3912\n",
      "arxiv-retrieval_cosine_recall@1: 0.0096\n",
      "arxiv-retrieval_cosine_recall@5: 0.0404\n",
      "arxiv-retrieval_cosine_recall@10: 0.0771\n",
      "arxiv-retrieval_cosine_ndcg@10: 0.4065\n",
      "arxiv-retrieval_cosine_mrr@10: 0.5780\n",
      "\n",
      "=== Improvement ===\n",
      "arxiv-retrieval_cosine_precision@1: -0.0361\n",
      "arxiv-retrieval_cosine_precision@5: -0.0363\n",
      "arxiv-retrieval_cosine_precision@10: -0.0237\n",
      "arxiv-retrieval_cosine_ndcg@10: -0.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate after fine-tuning\n",
    "print(\"=== Precision@k (after fine-tuning) ===\")\n",
    "final_results = ir_evaluator(model)\n",
    "for key, value in final_results.items():\n",
    "    if \"precision\" in key or \"recall\" in key or \"mrr\" in key or \"ndcg\" in key:\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Compare improvement\n",
    "print(\"\\n=== Improvement ===\")\n",
    "for key in baseline_results:\n",
    "    if \"precision\" in key or \"ndcg\" in key:\n",
    "        improvement = final_results[key] - baseline_results[key]\n",
    "        print(f\"{key}: {improvement:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
