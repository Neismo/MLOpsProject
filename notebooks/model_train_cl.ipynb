{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32481c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676efc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea4641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2039695\n",
      "Test samples: 509924\n",
      "Columns: ['primary_subject', 'subjects', 'abstract', 'title']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "from mlops_project.data import ArxivPapersDataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Load the dataset using the project's data module\n",
    "data_dir = Path(\"../data\")\n",
    "train_dataset = ArxivPapersDataset(split=\"train\", data_dir=data_dir).dataset\n",
    "test_dataset = ArxivPapersDataset(split=\"test\", data_dir=data_dir).dataset\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2jq29ptdehk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new pairs (will be cached at ../data/train_pairs)\n",
      "Found 148 unique subjects\n",
      "Creating 50000 positive pairs...\n",
      "Creating 50000 negative pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5557848c8d92434abfbcc71e5f5603ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training pairs: 100000\n",
      "{'sentence1': 'The paper describes the project, implementation and test of a C-band (5GHz) Low Noise Amplifier (LNA) using new low noise Pseudomorphic High Electron Mobility Transistors (pHEMTS) from Avago. The amplifier was developed to be used as a cost effective solution in a receiver chain for Galactic Emission Mapping (GEM-P) project in Portugal with the objective of finding affordable solutions not requiring strong cryogenic operation, as is the case of massive projects like the Square Kilometer Array (SKA), in Earth Sensing projects and other niches like microwave reflectometry. The particular application and amplifier requirements are first introduced. Several commercially available low noise devices were selected and the noise performance simulated. An ultra-low noise pHEMT was used for an implementation that achieved a Noise Figure of 0.6 dB with 13 dB gain at 5 GHz. The design, simulation and measured results of the prototype are presented and discussed.', 'sentence2': 'The image degradation produced by atmospheric turbulence and optical aberrations is usually alleviated using post-facto image reconstruction techniques, even when observing with adaptive optics systems. These techniques rely on the development of the wavefront using Zernike functions and the non-linear optimization of a certain metric. The resulting optimization procedure is computationally heavy. Our aim is to alleviate this computationally burden. To this aim, we generalize the recently developed extended Zernike-Nijboer theory to carry out the analytical integration of the Fresnel integral and present a natural basis set for the development of the point spread function in case the wavefront is described using Zernike functions. We present a linear expansion of the point spread function in terms of analytic functions which, additionally, takes defocusing into account in a natural way. This expansion is used to develop a very fast phase-diversity reconstruction technique which is demonstrated through some applications. This suggest that the linear expansion of the point spread function can be applied to accelerate other reconstruction techniques in use presently and based on blind deconvolution.', 'label': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def create_contrastive_pairs(dataset, num_pairs: int = 100000, text_field: str = \"abstract\", seed: int = 42):\n",
    "    \"\"\"\n",
    "    Create balanced positive and negative pairs for ContrastiveLoss.\n",
    "    \n",
    "    Returns a dataset with columns: sentence1, sentence2, label\n",
    "    - label=1.0 for positive pairs (same subject)\n",
    "    - label=0.0 for negative pairs (different subjects)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Group indices by subject\n",
    "    subject_to_indices = defaultdict(list)\n",
    "    for idx, subject in enumerate(dataset[\"primary_subject\"]):\n",
    "        subject_to_indices[subject].append(idx)\n",
    "    \n",
    "    subjects = list(subject_to_indices.keys())\n",
    "    print(f\"Found {len(subjects)} unique subjects\")\n",
    "    \n",
    "    pairs = {\"sentence1\": [], \"sentence2\": [], \"label\": []}\n",
    "    num_positive = num_pairs // 2\n",
    "    num_negative = num_pairs - num_positive\n",
    "    \n",
    "    # Create positive pairs (same subject)\n",
    "    print(f\"Creating {num_positive} positive pairs...\")\n",
    "    for _ in range(num_positive):\n",
    "        subject = random.choice([s for s in subjects if len(subject_to_indices[s]) >= 2])\n",
    "        idx1, idx2 = random.sample(subject_to_indices[subject], 2)\n",
    "        pairs[\"sentence1\"].append(dataset[idx1][text_field])\n",
    "        pairs[\"sentence2\"].append(dataset[idx2][text_field])\n",
    "        pairs[\"label\"].append(1.0)\n",
    "    \n",
    "    # Create negative pairs (different subjects)\n",
    "    print(f\"Creating {num_negative} negative pairs...\")\n",
    "    for _ in range(num_negative):\n",
    "        subj1, subj2 = random.sample(subjects, 2)\n",
    "        idx1 = random.choice(subject_to_indices[subj1])\n",
    "        idx2 = random.choice(subject_to_indices[subj2])\n",
    "        pairs[\"sentence1\"].append(dataset[idx1][text_field])\n",
    "        pairs[\"sentence2\"].append(dataset[idx2][text_field])\n",
    "        pairs[\"label\"].append(0.0)\n",
    "    \n",
    "    return Dataset.from_dict(pairs)\n",
    "\n",
    "def load_or_create_pairs(dataset, save_path: Path, num_pairs: int, text_field: str = \"abstract\", seed: int = 42):\n",
    "    \"\"\"Load pairs from disk if they exist, otherwise create and save them.\"\"\"\n",
    "    if save_path.exists():\n",
    "        print(f\"Loading cached pairs from {save_path}\")\n",
    "        return load_from_disk(str(save_path))\n",
    "    \n",
    "    print(f\"Creating new pairs (will be cached at {save_path})\")\n",
    "    pairs = create_contrastive_pairs(dataset, num_pairs=num_pairs, text_field=text_field, seed=seed)\n",
    "    pairs.save_to_disk(str(save_path))\n",
    "    return pairs\n",
    "\n",
    "# Create/load training pairs\n",
    "train_pairs_path = data_dir / \"train_pairs\"\n",
    "train_pairs = load_or_create_pairs(train_dataset, train_pairs_path, num_pairs=100000)\n",
    "print(f\"\\nTraining pairs: {len(train_pairs)}\")\n",
    "print(train_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6uqsqbt5oik",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new pairs (will be cached at ../data/eval_pairs)\n",
      "Found 148 unique subjects\n",
      "Creating 5000 positive pairs...\n",
      "Creating 5000 negative pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0322cf50c2c4e61a75b6b3b08f01f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation pairs: 10000\n"
     ]
    }
   ],
   "source": [
    "# Create/load evaluation pairs\n",
    "eval_pairs_path = data_dir / \"eval_pairs\"\n",
    "eval_pairs = load_or_create_pairs(test_dataset, eval_pairs_path, num_pairs=10000, seed=123)\n",
    "print(f\"Evaluation pairs: {len(eval_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "t00b0e1dfap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR Evaluator: 998 queries, 5000 corpus docs\n",
      "IR Evaluator created for precision@k metrics\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "import numpy as np\n",
    "\n",
    "def create_ir_evaluator(dataset, model, sample_size: int = 5000, name: str = \"arxiv-retrieval\"):\n",
    "    \"\"\"\n",
    "    Create an InformationRetrievalEvaluator for precision@k evaluation.\n",
    "    \n",
    "    For each query, relevant documents are those with the same primary_subject.\n",
    "    This measures how well the model retrieves papers from the same category.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Sample a subset for evaluation\n",
    "    indices = np.random.choice(len(dataset), min(sample_size, len(dataset)), replace=False)\n",
    "    \n",
    "    queries = {}  # query_id -> query text\n",
    "    corpus = {}   # corpus_id -> document text\n",
    "    relevant_docs = {}  # query_id -> set of relevant corpus_ids\n",
    "    \n",
    "    # Build corpus and group by subject\n",
    "    subject_to_corpus_ids = defaultdict(set)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        idx = int(idx)\n",
    "        corpus_id = f\"doc_{i}\"\n",
    "        corpus[corpus_id] = dataset[idx][\"abstract\"]\n",
    "        subject = dataset[idx][\"primary_subject\"]\n",
    "        subject_to_corpus_ids[subject].add(corpus_id)\n",
    "    \n",
    "    # Use a subset as queries, rest as corpus for retrieval\n",
    "    query_indices = indices[:sample_size // 5]  # 20% as queries\n",
    "    \n",
    "    for i, idx in enumerate(query_indices):\n",
    "        idx = int(idx)\n",
    "        query_id = f\"query_{i}\"\n",
    "        queries[query_id] = dataset[idx][\"abstract\"]\n",
    "        subject = dataset[idx][\"primary_subject\"]\n",
    "        # Relevant docs are all docs with same subject (excluding self)\n",
    "        doc_id = f\"doc_{i}\"\n",
    "        relevant_docs[query_id] = subject_to_corpus_ids[subject] - {doc_id}\n",
    "    \n",
    "    # Filter out queries with no relevant docs\n",
    "    queries = {qid: q for qid, q in queries.items() if len(relevant_docs.get(qid, set())) > 0}\n",
    "    relevant_docs = {qid: docs for qid, docs in relevant_docs.items() if qid in queries}\n",
    "    \n",
    "    print(f\"IR Evaluator: {len(queries)} queries, {len(corpus)} corpus docs\")\n",
    "    \n",
    "    return InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=name,\n",
    "        precision_recall_at_k=[1, 5, 10],\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "# Create the IR evaluator for precision@k\n",
    "ir_evaluator = create_ir_evaluator(test_dataset, model, sample_size=5000)\n",
    "print(\"IR Evaluator created for precision@k metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61jd6zeqsq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Precision@k (before fine-tuning) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a401722d145e4a8aabb2550d9dbb4834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5814d22bd7f4e97b1565f15432dc838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv-retrieval_cosine_precision@1: 0.0000\n",
      "arxiv-retrieval_cosine_precision@5: 0.3741\n",
      "arxiv-retrieval_cosine_precision@10: 0.3905\n",
      "arxiv-retrieval_cosine_recall@1: 0.0000\n",
      "arxiv-retrieval_cosine_recall@5: 0.0323\n",
      "arxiv-retrieval_cosine_recall@10: 0.0620\n",
      "arxiv-retrieval_cosine_ndcg@10: 0.3473\n",
      "arxiv-retrieval_cosine_mrr@10: 0.3461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline (before fine-tuning)\n",
    "print(\"=== Baseline Precision@k (before fine-tuning) ===\")\n",
    "baseline_results = ir_evaluator(model)\n",
    "for key, value in baseline_results.items():\n",
    "    if \"precision\" in key or \"recall\" in key or \"mrr\" in key or \"ndcg\" in key:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "j9niui5kjqg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9d53442d8a4525839cc8feef67fba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized with precision@k evaluator. Ready to train!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.losses import ContrastiveLoss\n",
    "\n",
    "# Define training arguments\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"../models/contrastive-minilm\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Initialize loss function\n",
    "loss = ContrastiveLoss(model)\n",
    "\n",
    "# Create trainer with IR evaluator for precision@k\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_pairs,\n",
    "    eval_dataset=eval_pairs,\n",
    "    loss=loss,\n",
    "    evaluator=ir_evaluator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized with precision@k evaluator. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "icdba4mnsyh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthorhojhus\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thorh/MLOpsProject/notebooks/wandb/run-20260109_145106-pjcp0ekh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thorhojhus/sentence-transformers/runs/pjcp0ekh' target=\"_blank\">misty-pyramid-3</a></strong> to <a href='https://wandb.ai/thorhojhus/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thorhojhus/sentence-transformers' target=\"_blank\">https://wandb.ai/thorhojhus/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thorhojhus/sentence-transformers/runs/pjcp0ekh' target=\"_blank\">https://wandb.ai/thorhojhus/sentence-transformers/runs/pjcp0ekh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 04:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Arxiv-retrieval Cosine Accuracy@1</th>\n",
       "      <th>Arxiv-retrieval Cosine Accuracy@3</th>\n",
       "      <th>Arxiv-retrieval Cosine Accuracy@5</th>\n",
       "      <th>Arxiv-retrieval Cosine Accuracy@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Precision@1</th>\n",
       "      <th>Arxiv-retrieval Cosine Precision@5</th>\n",
       "      <th>Arxiv-retrieval Cosine Precision@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Recall@1</th>\n",
       "      <th>Arxiv-retrieval Cosine Recall@5</th>\n",
       "      <th>Arxiv-retrieval Cosine Recall@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Ndcg@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Mrr@10</th>\n",
       "      <th>Arxiv-retrieval Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.014209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587174</td>\n",
       "      <td>0.728457</td>\n",
       "      <td>0.845691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334469</td>\n",
       "      <td>0.355210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027520</td>\n",
       "      <td>0.056093</td>\n",
       "      <td>0.314754</td>\n",
       "      <td>0.318050</td>\n",
       "      <td>0.175042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601202</td>\n",
       "      <td>0.729459</td>\n",
       "      <td>0.844689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336874</td>\n",
       "      <td>0.359619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028562</td>\n",
       "      <td>0.057467</td>\n",
       "      <td>0.318682</td>\n",
       "      <td>0.320458</td>\n",
       "      <td>0.182369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590180</td>\n",
       "      <td>0.724449</td>\n",
       "      <td>0.840681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336273</td>\n",
       "      <td>0.359519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027743</td>\n",
       "      <td>0.057915</td>\n",
       "      <td>0.318382</td>\n",
       "      <td>0.317599</td>\n",
       "      <td>0.184057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175ebf73802945ffb62e9249c6d6aa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9b69d2f28b4020906a7f930df7cc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b572c94f91654dcd8f23f0da080aa9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77687517f2ff4a05a140cb2df03b0fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b095447ab0054072a9f733fea80ac45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451aac5098d84fbba1e99a4357f8fe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1563, training_loss=0.01710545784071021, metrics={'train_runtime': 259.6964, 'train_samples_per_second': 385.065, 'train_steps_per_second': 6.019, 'total_flos': 0.0, 'train_loss': 0.01710545784071021, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "yhukcae0z1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Precision@k (after fine-tuning) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49ed7d7ab1541c7a6f4759435896aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff819d8ee280443fbbfb97679bfb4130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv-retrieval_cosine_precision@1: 0.0000\n",
      "arxiv-retrieval_cosine_precision@5: 0.3369\n",
      "arxiv-retrieval_cosine_precision@10: 0.3602\n",
      "arxiv-retrieval_cosine_recall@1: 0.0000\n",
      "arxiv-retrieval_cosine_recall@5: 0.0280\n",
      "arxiv-retrieval_cosine_recall@10: 0.0581\n",
      "arxiv-retrieval_cosine_ndcg@10: 0.3190\n",
      "arxiv-retrieval_cosine_mrr@10: 0.3183\n",
      "\n",
      "=== Improvement ===\n",
      "arxiv-retrieval_cosine_precision@1: +0.0000\n",
      "arxiv-retrieval_cosine_precision@5: -0.0373\n",
      "arxiv-retrieval_cosine_precision@10: -0.0303\n",
      "arxiv-retrieval_cosine_ndcg@10: -0.0283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate after fine-tuning\n",
    "print(\"=== Precision@k (after fine-tuning) ===\")\n",
    "final_results = ir_evaluator(model)\n",
    "for key, value in final_results.items():\n",
    "    if \"precision\" in key or \"recall\" in key or \"mrr\" in key or \"ndcg\" in key:\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Compare improvement\n",
    "print(\"\\n=== Improvement ===\")\n",
    "for key in baseline_results:\n",
    "    if \"precision\" in key or \"ndcg\" in key:\n",
    "        improvement = final_results[key] - baseline_results[key]\n",
    "        print(f\"{key}: {improvement:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-project (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
